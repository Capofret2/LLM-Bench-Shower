# LLM Bench Shower 缓存架构文档

## 概述

LLM Bench Shower 采用**双层缓存架构**，包括：
1. **结果缓存**：存储评测结果，避免重复运行相同测试
2. **模型缓存**：管理模型加载，提高推理效率

两种缓存都采用**内存 + 持久化**的混合策略，既保证快速访问，又确保数据不丢失。

---

## 一、结果缓存（Benchmark Results Cache）

### 1.1 架构设计

结果缓存采用**内存缓存 + SQLite 数据库**的双层架构：

```
┌─────────────────────────────────────────┐
│  内存缓存 (bench_history)                │
│  - 快速访问                              │
│  - 启动时从数据库加载                     │
│  - 运行时直接返回                        │
└─────────────────────────────────────────┘
           │                    │
           │ 读取               │ 写入
           ▼                    ▼
┌─────────────────────────────────────────┐
│  SQLite 数据库 (benchmark_results.db)   │
│  - 持久化存储                            │
│  - 异步批量写回                          │
│  - 服务重启后自动恢复                    │
└─────────────────────────────────────────┘
```

### 1.2 缓存键设计

使用 `(model_name, dataset_name)` 作为唯一键：

```python
class ModelDatasetPair(NamedTuple):
    model_name: str      # 模型路径或名称
    dataset_name: str    # 数据集名称，格式：dataset/subdataset
```

**示例**：
- `("/root/models/llama-7b", "C-Eval/computer_network")`
- `("api::gpt-4", "MR-GMS8K/MR-GMS8K")`

### 1.3 缓存流程

#### 读取流程

```
1. 检查内存缓存 (bench_history)
   ├─ 命中 → 直接返回结果（最快）
   └─ 未命中 → 继续步骤 2

2. 检查数据库
   ├─ 命中 → 加载到内存缓存 → 返回结果
   └─ 未命中 → 运行评测 → 保存到缓存 → 返回结果
```

#### 写入流程

```
1. 评测完成后，结果写入内存缓存
   └─ bench_history[pair] = results

2. 标记为"脏数据"（需要写回数据库）
   └─ _dirty_results.add(pair)

3. 后台线程异步批量写回（每 240 秒）
   └─ _writeback_worker() → _flush_dirty_results()
```

### 1.4 数据库设计

**表结构**：`benchmark_results`

| 字段 | 类型 | 说明 |
|------|------|------|
| `id` | INTEGER | 主键，自增 |
| `model_name` | TEXT | 模型名称/路径 |
| `dataset_name` | TEXT | 数据集名称 |
| `results_json` | TEXT | 评测结果（JSON 格式） |
| `created_at` | TIMESTAMP | 创建时间 |
| `updated_at` | TIMESTAMP | 更新时间（自动更新） |

**索引**：
- `(model_name, dataset_name)` 唯一索引，确保不重复

**触发器**：
- `update_timestamp`：更新时自动更新 `updated_at`

### 1.5 写回机制

**写回策略**：异步批量写回

- **写回间隔**：默认 240 秒（`LBS_DB_WRITEBACK_S`）
- **写回方式**：批量事务写入，提高性能
- **容错机制**：写回失败时，重新标记为脏数据，下次重试

**优势**：
- 减少数据库 I/O 操作
- 批量写入提高性能
- 不影响评测主流程

### 1.6 缓存失效策略

**自动失效**：
- 相同 `(model_name, dataset_name)` 的新结果会覆盖旧结果
- 数据库使用 `ON CONFLICT ... DO UPDATE` 实现

**手动清除**：
- 通过 API 删除：`DELETE /api/history?model_name=xxx&dataset_name=xxx`
- 清除所有：`DELETE /api/history`

**特殊处理**：
- `NeedleInAHaystack` 数据集：强制重新运行（忽略缓存），因为需要参数化测试

### 1.7 配置参数

| 环境变量 | 默认值 | 说明 |
|---------|--------|------|
| `LBS_DB_PATH` | `backend/data/benchmark_results.db` | 数据库文件路径 |
| `LBS_DB_WRITEBACK_S` | `240` | 写回间隔（秒） |

---

## 二、模型缓存（Model Cache）

### 2.1 架构设计

模型缓存采用**GPU + CPU 两级缓存**架构：

```
┌─────────────────────────────────────────┐
│  GPU 缓存 (gpu_cache)                    │
│  - 模型在 GPU 上，推理最快                │
│  - 受 GPU 显存限制                       │
│  - 优先使用                               │
└─────────────────────────────────────────┘
           │                    │
           │ 显存不足时         │ 需要时
           │ 降级到 CPU         │ 提升到 GPU
           ▼                    ▼
┌─────────────────────────────────────────┐
│  CPU 缓存 (cpu_cache)                   │
│  - 模型在 CPU 上，节省显存                │
│  - 受 CPU 内存限制                       │
│  - 需要时再加载到 GPU                     │
└─────────────────────────────────────────┘
```

### 2.2 缓存键设计

使用**模型路径**作为唯一键：

```python
model_name_or_path: str  # 例如："/root/models/llama-7b"
```

### 2.3 缓存流程

#### 获取模型流程

```
1. 检查 GPU 缓存
   ├─ 命中 → 更新访问统计 → 直接返回（最快）
   └─ 未命中 → 继续步骤 2

2. 检查 CPU 缓存
   ├─ 命中 → 加载到 GPU → 更新缓存 → 返回
   └─ 未命中 → 继续步骤 3

3. 从磁盘加载模型
   ├─ 检查显存/内存限制
   ├─ 必要时淘汰旧模型
   ├─ 加载模型到 GPU
   └─ 添加到 GPU 缓存 → 返回
```

#### 淘汰策略（LRU + 智能评分）

当缓存满时，使用**多因素评分**选择淘汰候选：

**评分因素**：
1. **访问频率**（40% 权重）：访问次数越多，优先级越高
2. **访问时间**（35% 权重）：最近访问的模型，优先级越高
3. **模型大小**（15% 权重）：小模型更容易重新加载，优先淘汰
4. **缓存时间**（10% 权重）：新加载的模型有短暂保护期

**评分公式**：
```python
score = (
    frequency_score * 0.4 +
    recency_score * 0.35 +
    size_score * 0.15 +
    newness_bonus * 0.1
)
```

**淘汰顺序**：
1. 优先从 CPU 缓存淘汰（释放内存）
2. CPU 缓存空时，从 GPU 缓存淘汰到 CPU（降级）
3. CPU 缓存也满时，从 CPU 缓存完全删除

### 2.4 内存管理

#### GPU 显存管理

- **最大利用率**：默认 50%（`LBS_GPU_MAX_UTILIZATION`）
- **显存监控**：实时跟踪已用显存
- **自动清理**：显存不足时自动淘汰模型

#### CPU 内存管理

- **最大利用率**：默认 80%（`LBS_CPU_MAX_UTILIZATION`）
- **内存监控**：使用 `psutil` 跟踪内存使用
- **自动清理**：内存不足时自动淘汰模型

### 2.5 模型移动机制

#### GPU → CPU（降级）

当 GPU 显存不足时：
1. 移除 accelerate hooks（如果使用 `device_map="auto"`）
2. 将模型移动到 CPU：`model.to("cpu")`
3. 从 `gpu_cache` 移除，添加到 `cpu_cache`
4. 清理 GPU 显存：`torch.cuda.empty_cache()`

#### CPU → GPU（提升）

当需要 GPU 推理时：
1. 从 `cpu_cache` 获取模型
2. 恢复 device_map（如果之前保存）
3. 使用 `dispatch_model` 或 `model.to("cuda")` 加载到 GPU
4. 从 `cpu_cache` 移除，添加到 `gpu_cache`

### 2.6 配置参数

| 环境变量 | 默认值 | 说明 |
|---------|--------|------|
| `LBS_USE_MODEL_CACHE` | `1` | 是否启用模型缓存 |
| `LBS_MAX_CACHED_LOCAL_MODELS` | `4` | 最大缓存模型数量 |
| `LBS_GPU_MAX_UTILIZATION` | `0.5` | GPU 最大利用率（0.0-1.0） |
| `LBS_CPU_MAX_UTILIZATION` | `0.8` | CPU 最大利用率（0.0-1.0） |
| `LBS_LOCAL_DEVICE_MAP` | `"auto"` | 设备映射（auto/cuda:0/cpu） |

---

## 三、缓存交互流程

### 3.1 完整评测流程

```
用户请求评测
    │
    ▼
检查结果缓存（内存）
    │
    ├─ 命中 → 直接返回结果 ✅
    │
    └─ 未命中
        │
        ▼
    检查结果缓存（数据库）
        │
        ├─ 命中 → 加载到内存 → 返回结果 ✅
        │
        └─ 未命中
            │
            ▼
        检查模型缓存
            │
            ├─ GPU 缓存命中 → 使用模型 ✅
            ├─ CPU 缓存命中 → 加载到 GPU → 使用模型 ✅
            └─ 未命中 → 从磁盘加载 → 添加到缓存 → 使用模型 ✅
            │
            ▼
        运行评测
            │
            ▼
        保存结果到内存缓存
            │
            ▼
        标记为脏数据（异步写回数据库）
            │
            ▼
        返回结果 ✅
```

### 3.2 缓存一致性

**结果缓存**：
- 写入时：先更新内存，再异步写回数据库
- 读取时：优先从内存读取，未命中时从数据库加载
- 一致性：内存是"热数据"，数据库是"持久化备份"

**模型缓存**：
- 单进程内缓存，无需考虑多进程一致性
- 模型文件不变，缓存键（路径）唯一

---

## 四、性能优化

### 4.1 结果缓存优化

1. **内存优先**：所有读取优先从内存获取，避免数据库 I/O
2. **批量写回**：多个结果合并写入，减少数据库操作
3. **异步写回**：不阻塞评测主流程
4. **索引优化**：数据库使用唯一索引，快速查找

### 4.2 模型缓存优化

1. **两级缓存**：GPU 缓存快速推理，CPU 缓存节省显存
2. **智能淘汰**：多因素评分，保留最有价值的模型
3. **懒加载**：需要时才加载到 GPU，节省显存
4. **内存监控**：实时监控，自动管理

---

## 五、使用建议

### 5.1 结果缓存

**适用场景**：
- ✅ 重复评测相同模型和数据集
- ✅ 需要历史结果对比
- ✅ 服务重启后恢复结果

**不适用场景**：
- ❌ 需要强制重新评测（可手动清除缓存）
- ❌ NeedleInAHaystack 参数化测试（自动忽略缓存）

### 5.2 模型缓存

**推荐配置**（RTX 4090 24GB）：
```bash
export LBS_USE_MODEL_CACHE=1
export LBS_MAX_CACHED_LOCAL_MODELS=2  # 根据模型大小调整
export LBS_GPU_MAX_UTILIZATION=0.9   # 使用 90% 显存
export LBS_CPU_MAX_UTILIZATION=0.8   # 使用 80% 内存
```

**小显存配置**（< 16GB）：
```bash
export LBS_MAX_CACHED_LOCAL_MODELS=1  # 只缓存一个模型
export LBS_GPU_MAX_UTILIZATION=0.8   # 保守使用显存
```

**大显存配置**（> 32GB）：
```bash
export LBS_MAX_CACHED_LOCAL_MODELS=4  # 可以缓存更多模型
export LBS_GPU_MAX_UTILIZATION=0.95  # 充分利用显存
```

---

## 六、故障处理

### 6.1 数据库损坏

如果数据库文件损坏：
1. 备份损坏的数据库文件
2. 删除数据库文件，系统会自动重建
3. 内存缓存不受影响，但重启后会丢失

### 6.2 显存不足

如果遇到 OOM（Out of Memory）：
1. 降低 `LBS_MAX_CACHED_LOCAL_MODELS`
2. 降低 `LBS_GPU_MAX_UTILIZATION`
3. 手动清除模型缓存：重启服务

### 6.3 缓存失效

如果缓存不生效：
1. 检查环境变量是否正确设置
2. 检查数据库文件权限
3. 查看日志确认缓存命中情况

---

## 七、API 接口

### 7.1 结果缓存 API

**获取所有历史结果**：
```http
GET /api/history
```

**删除特定结果**：
```http
DELETE /api/history?model_name=xxx&dataset_name=xxx
```

**清除所有结果**：
```http
DELETE /api/history
```

**获取数据库统计**：
```http
GET /api/database/stats
```

### 7.2 模型缓存

模型缓存通过环境变量配置，无直接 API 接口。

---

## 八、总结

### 8.1 缓存优势

1. **性能提升**：
   - 结果缓存：避免重复评测，节省时间
   - 模型缓存：避免重复加载，节省时间

2. **资源优化**：
   - 结果缓存：减少计算资源消耗
   - 模型缓存：智能管理显存和内存

3. **用户体验**：
   - 快速返回历史结果
   - 支持结果对比和分析

### 8.2 设计原则

1. **内存优先**：快速访问，延迟持久化
2. **异步写回**：不阻塞主流程
3. **智能淘汰**：保留最有价值的数据
4. **容错机制**：写回失败自动重试

### 8.3 未来改进

1. **结果缓存**：
   - 支持缓存版本控制
   - 支持部分结果缓存（增量评测）
   - 支持缓存压缩

2. **模型缓存**：
   - 支持模型量化缓存（8-bit/4-bit）
   - 支持分布式缓存
   - 支持缓存预热

---

## 附录：相关文件

- **结果缓存**：
  - `backend/runner.py`：结果缓存逻辑
  - `backend/db.py`：数据库操作
  - `backend/envs.py`：配置参数

- **模型缓存**：
  - `backend/model_cache.py`：模型缓存实现
  - `backend/runner.py`：缓存使用逻辑

