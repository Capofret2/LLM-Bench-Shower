import os
import json
import torch
import re
from typing import Dict, List, Any
from transformers import AutoModelForCausalLM, AutoTokenizer
from datasets import load_dataset
from openai import Client
from ..benchbase import BaseBench
from ..utils import get_dataset_path, get_sub_datasets

class C_EvalBenchmarker(BaseBench):
    """Benchmarker for CEval dataset.
    
    C-Eval is a comprehensive Chinese evaluation suite for foundation models.
    It consists of multiple-choice questions across various disciplines.
    Reference: https://huggingface.co/datasets/ceval/ceval-exam
    """
    def __init__(self):
        """Initialize the CEval benchmarker."""
        self.dataset_path = get_dataset_path("C-Eval")
        self.sub_datasets = get_sub_datasets("C-Eval")

    def _load_dataset(self, subdataset_name: str) -> List[Dict]:
        """Load a subdataset from local files or Hugging Face.

        Args:
            subdataset_name (str): The name of the subdataset (subject) to load.

        Returns:
            List[Dict]: The loaded dataset as a list of dictionaries.
        """
        # Â¶ÇÊûú subdataset_name ÊòØ "C-Eval"ÔºåÊèêÁ§∫Áî®Êà∑ÈÄâÊã©ÂÖ∑‰ΩìÁöÑÁßëÁõÆ
        if subdataset_name == "C-Eval":
            available_subjects = [f for f in os.listdir(self.dataset_path) 
                                 if f.endswith('.jsonl') and os.path.isfile(os.path.join(self.dataset_path, f))]
            available_subjects = [f.replace('.jsonl', '') for f in available_subjects]
            error_msg = (
                f"C-Eval requires a specific subject name, not 'C-Eval'.\n"
                f"Available subjects in {self.dataset_path}:\n"
                f"  {', '.join(sorted(available_subjects)[:10])}{'...' if len(available_subjects) > 10 else ''}\n"
                f"\n"
                f"Please use format: C-Eval/{{subject_name}}\n"
                f"Example: C-Eval/computer_network, C-Eval/high_school_mathematics"
            )
            raise ValueError(error_msg)
        
        # ‰ºòÂÖàÂ∞ùËØï‰ªéÊµãËØïÊï∞ÊçÆË∑ØÂæÑÂä†ËΩΩ
        # ‰ªé benchmarker.py ÂõûÂà∞È°πÁõÆÊ†πÁõÆÂΩï: backend/bench/c_eval -> backend/bench -> backend -> LLMBenchShower -> È°πÁõÆÊ†π
        test_data_base = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))), "tests", "test_data")
        
        # ÊûÑÂª∫ÊâÄÊúâÂèØËÉΩÁöÑË∑ØÂæÑÔºàÊµãËØïÊï∞ÊçÆ‰ºòÂÖàÔºâ
        local_paths = [
            # ÊµãËØïÊï∞ÊçÆË∑ØÂæÑÔºötests/test_data/C-Eval/{subject}.jsonl
            os.path.join(test_data_base, "C-Eval", f"{subdataset_name}.jsonl"),
            # Áîü‰∫ßÊï∞ÊçÆË∑ØÂæÑÔºö/root/share/datasets/C-Eval/{subject}.jsonl
            os.path.join(self.dataset_path, f"{subdataset_name}.jsonl"),
        ]
        
        local_file = None
        for path in local_paths:
            if os.path.exists(path):
                local_file = path
                print(f"[C-Eval] ‚úÖ Found dataset at: {local_file}")
                break
        
        if local_file:
            try:
                data_list = []
                with open(local_file, 'r', encoding='utf-8') as f:
                    for line in f:
                        if line.strip():
                            item = json.loads(line)
                            # Á°Æ‰øù subject_name Â≠óÊÆµÂ≠òÂú®
                            if 'subject_name' not in item:
                                item['subject_name'] = subdataset_name
                            data_list.append(item)
                print(f"[C-Eval] ‚úÖ Loaded {len(data_list)} items from local file: {local_file}")
                return data_list
            except Exception as e:
                print(f"[C-Eval] ‚ö†Ô∏è  Failed to load from local file {local_file}: {e}")
                print(f"[C-Eval] Falling back to Hugging Face...")
        else:
            print(f"[C-Eval] ‚ö†Ô∏è  Local file not found in any of the paths:")
            for path in local_paths:
                print(f"  - {path}")
            print(f"[C-Eval] Attempting to load from Hugging Face...")
        
        # Â¶ÇÊûúÊú¨Âú∞Êñá‰ª∂‰∏çÂ≠òÂú®ÊàñÂä†ËΩΩÂ§±Ë¥•ÔºåÂ∞ùËØï‰ªé Hugging Face Âä†ËΩΩ
        # Ê≥®ÊÑèÔºöÁßªÈô§ trust_remote_codeÔºåÂõ†‰∏∫Êñ∞ÁâàÊú¨ÁöÑ datasets Â∫ì‰∏çÂÜçÊîØÊåÅ
        hf_dataset_names = [
            "ceval/ceval-exam",  # Hugging Face Êï∞ÊçÆÈõÜÂêçÁß∞
        ]
        
        for hf_path in hf_dataset_names:
            try:
                print(f"[C-Eval] üì• Attempting to load from Hugging Face: {hf_path}")
                # path: ËøúÁ®ã‰ªìÂ∫ìÂú∞ÂùÄ
                # name: ÂÖ∑‰ΩìÁßëÁõÆ (e.g.'computer_network')
                # split: C-EvalÈÄöÂ∏∏È™åËØÅÁî® 'val' (testÈõÜÊó†Á≠îÊ°à)
                # cache_dir: ÊåáÂÆöÊú¨Âú∞ÁºìÂ≠òÁõÆÂΩï
                dataset = load_dataset(
                    path=hf_path,
                    name=subdataset_name,
                    split="val",
                    cache_dir=self.dataset_path
                )
                data_list = []
                for item in dataset:
                    item_dict = dict(item)
                    item_dict['subject_name'] = subdataset_name
                    data_list.append(item_dict)
                print(f"[C-Eval] ‚úÖ Successfully loaded {len(data_list)} items from Hugging Face")
                return data_list
            except Exception as e:
                print(f"[C-Eval] ‚ùå Failed to load from {hf_path}: {e}")
                continue
        
        # Â¶ÇÊûúÊâÄÊúâÊñπÊ≥ïÈÉΩÂ§±Ë¥•ÔºåÊèê‰æõËØ¶ÁªÜÁöÑÈîôËØØ‰ø°ÊÅØ
        error_msg = (
            f"Failed to load dataset '{subdataset_name}' for C-Eval.\n"
            f"Tried:\n"
            f"  1. Local file: {local_file} ({'not found' if not os.path.exists(local_file) else 'load failed'})\n"
            f"  2. Hugging Face dataset: {hf_dataset_names[0]} (failed)\n"
            f"\n"
            f"Solutions:\n"
            f"  - Ensure the local dataset file exists: {local_file}\n"
            f"  - Download the dataset using: python /root/share/datasets/C-Eval/download_ceval.py --subjects {subdataset_name}\n"
            f"  - Or ensure you have internet access to download from Hugging Face\n"
            f"  - Check that the dataset name '{subdataset_name}' is correct"
        )
        raise RuntimeError(error_msg)

    def _prepare_prompt(self, item: Dict) -> str:
        """Prepare the prompt from a dataset item (MCQ format).

        Args:
            item (Dict): A data item containing question and options.

        Returns:
            str: The formatted prompt.
        """
        question = item.get("question", "")
        opt_a = item.get("A", "")
        opt_b = item.get("B", "")
        opt_c = item.get("C", "")
        opt_d = item.get("D", "")
        
        # ÁÆÄÂçïÁöÑÈÄâÊã©È¢òpromptÊ®°Êùø
        prompt = (
            f"\n\nQuestion:{question}\nA. {opt_a}\nB. {opt_b}\nC. {opt_c}\nD. {opt_d}\n\nAnswer:"
        )
        # Only print prompt in debug mode (commented out to reduce log noise)
        # print("[debug] prompt prepared:", prompt)
        return prompt

    def _extract_prediction_label(self, prediction: str) -> str:
        """Extract the option letter (A/B/C/D) from model output.
        
        Args:
            prediction (str): The raw model output.
            
        Returns:
            str: The extracted letter or empty string.
        """
        # Ê∏ÖÁêÜÁ©∫ÁôΩÂ≠óÁ¨¶
        pred = prediction.strip()
        if not pred:
            return ""
            
        # 1. Â¶ÇÊûúÁ¨¨‰∏Ä‰∏™Â≠óÁ¨¶Â∞±ÊòØÈÄâÈ°π
        if pred[0].upper() in ["A", "B", "C", "D"]:
            return pred[0].upper()
            
        # 2. Ê≠£Âàô
        match = re.search(r"(?:Á≠îÊ°à|ÈÄâ|Choice|Answer)[\s:Ôºö]*([ABCD])", pred, re.IGNORECASE)
        if match:
            return match.group(1).upper()
            
        # 3. Â¶ÇÊûúÊ≤°ÊâæÂà∞ÊòéÁ°ÆÊ†áËÆ∞ÔºåÂ∞ùËØïÊâæÊúÄÂêé‰∏Ä‰∏™Âá∫Áé∞ÁöÑÈÄâÈ°πÂ≠óÊØç
        matches = re.findall(r"([ABCD])", pred.upper())
        if matches:
            return matches[-1]
            
        return ""

    def _calculate_score(self, prediction: str, ground_truth: str) -> float:
        """Calculate score (0 or 1) for MCQ.

        Args:
            prediction (str): The model's raw prediction text.
            ground_truth (str): The correct option letter (e.g., "A").

        Returns:
            float: 1.0 if correct, 0.0 otherwise.
        """
        extracted_pred = self._extract_prediction_label(prediction)
        
        # ÊØîËæÉÊèêÂèñÂá∫ÁöÑÁ≠îÊ°àÂíåÊ†áÂáÜÁ≠îÊ°à
        if extracted_pred == ground_truth.strip().upper():
            return 1.0
        return 0.0

    def evaluate_local_llm(
        self,
        model: AutoModelForCausalLM,
        tokenizer: AutoTokenizer,
        subdataset_name: str,
        *args,
        **kwargs,
    ) -> Dict:
        """Evaluate a local LLM on CEval dataset."""
        dataset = self._load_dataset(subdataset_name)
        
        results = {
            "dataset": subdataset_name,
            "model_type": "local",
            "predictions": [],
            "metrics": {
                "total": len(dataset),
                "processed": 0,
                "score": 0.0,
            }
        }
        
        all_scores = []
        total_items = len(dataset)
        
        # Get progress callback if available
        progress_callback = kwargs.get("progress_callback")
        
        print(f"[C-Eval] Starting evaluation on {total_items} items...")
        print(f"[C-Eval] Model device: {next(model.parameters()).device}")
        print(f"[C-Eval] Model dtype: {next(model.parameters()).dtype}")
        
        # Initialize progress
        if progress_callback:
            progress_callback(0, total_items, "ÂàùÂßãÂåñ‰∏≠...")
        
        import time
        start_time = time.time()
        
        for idx, item in enumerate(dataset, 1):
            # Print progress every 5 items or on first/last item
            if idx == 1 or idx == total_items or idx % 5 == 0:
                print(f"[C-Eval] Processing item {idx}/{total_items} ({idx*100//total_items}%)")
            
            # Update progress with current question
            question_text = item.get("question", "")[:100]  # Limit question length
            if progress_callback:
                progress_callback(idx, total_items, question_text)
            
            try:
                item_start_time = time.time()
                prompt = self._prepare_prompt(item)
                inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=4096)
                inputs = {k: v.to(model.device) for k, v in inputs.items()}
                
                with torch.no_grad():
                    # For multiple-choice questions, use greedy decoding for faster inference
                    # Greedy decoding (temperature=0, do_sample=False) is faster and deterministic
                    outputs = model.generate(
                        **inputs,
                        max_new_tokens=128,  # Reduced from 512: multiple-choice answers are short
                        temperature=0.0,    # Greedy decoding for speed
                        do_sample=False,    # Disable sampling for faster inference
                        pad_token_id=tokenizer.pad_token_id or tokenizer.eos_token_id,
                    )
                
                item_time = time.time() - item_start_time
                if idx <= 3 or idx % 10 == 0:  # Log timing for first 3 items and every 10th item
                    print(f"[C-Eval] Item {idx} processed in {item_time:.2f}s")
                
                # Extract only the generated text (excluding the prompt)
                # Method 1: Decode only new tokens (preferred)
                try:
                    if "input_ids" in inputs and isinstance(inputs["input_ids"], torch.Tensor):
                        input_length = inputs["input_ids"].shape[1]
                        # Decode only the newly generated tokens
                        response = tokenizer.decode(outputs[0][input_length:], skip_special_tokens=True)
                    else:
                        # Fallback: decode full output and remove prompt
                        full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)
                        response = full_response[len(prompt):] if full_response.startswith(prompt) else full_response
                except Exception as e:
                    # Fallback: decode full output and try to remove prompt
                    print(f"[C-Eval] Warning: Error extracting new tokens: {e}, using fallback method")
                    full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)
                    response = full_response[len(prompt):] if full_response.startswith(prompt) else full_response
                
                response = response.strip()
                print(f"[C-Eval] Generated response: {response[:100]}..." if len(response) > 100 else f"[C-Eval] Generated response: {response}")
                
                ground_truth = item.get("answer", "")
                
                # Calculate score
                score = self._calculate_score(response, ground_truth)
                all_scores.append(score)
                
                results["predictions"].append({
                    "question": item.get("question", ""),
                    "prediction": response, 
                    "extracted_answer": self._extract_prediction_label(response),
                    "ground_truth": ground_truth,
                    "score": score,
                })
                results["metrics"]["processed"] += 1
                
            except Exception as e:
                print(f"Error processing item: {e}")
                results["predictions"].append({
                    "question": item.get("question", ""),
                    "prediction": f"Error: {str(e)}",
                    "ground_truth": item.get("answer", ""),
                    "score": 0.0,
                })
        
        total_time = time.time() - start_time
        avg_time_per_item = total_time / total_items if total_items > 0 else 0
        
        if all_scores:
            results["metrics"]["score"] = sum(all_scores) / len(all_scores)
            accuracy = results["metrics"]["score"] * 100
            print(f"[C-Eval] ‚úÖ Evaluation completed: {results['metrics']['processed']}/{total_items} items processed, Accuracy: {accuracy:.2f}%")
            print(f"[C-Eval] ‚è±Ô∏è  Performance: Total time: {total_time:.1f}s, Average: {avg_time_per_item:.2f}s/item, Speed: {total_items/total_time:.2f} items/s")
        else:
            print(f"[C-Eval] ‚ö†Ô∏è Evaluation completed: {results['metrics']['processed']}/{total_items} items processed, but no scores calculated")
            print(f"[C-Eval] ‚è±Ô∏è  Performance: Total time: {total_time:.1f}s, Average: {avg_time_per_item:.2f}s/item")
        
        return results

    def evaluate_api_llm(
        self,
        client: Client,
        model: str,
        subdataset_name: str,
        *args,
        **kwargs,
    ) -> Dict:
        """Evaluate an API LLM on CEval dataset."""
        dataset = self._load_dataset(subdataset_name)
        
        results = {
            "dataset": subdataset_name,
            "model_type": "api",
            "model": model,
            "predictions": [],
            "metrics": {
                "total": len(dataset),
                "processed": 0,
                "score": 0.0,
            }
        }
        
        all_scores = []
        
        for item in dataset:
            try:
                prompt = self._prepare_prompt(item)
                
                response = client.messages.create(
                    model=model,
                    messages=[
                        {
                            "role": "user",
                            "content": prompt,
                        }
                    ],
                    max_tokens=512,
                    temperature=0.7,
                )
                #print("[debug api] response:", response)
                prediction = response.choices[0].message.content.strip()
                ground_truth = item.get("answer", "")
                
                score = self._calculate_score(prediction, ground_truth)
                all_scores.append(score)
                
                results["predictions"].append({
                    "question": item.get("question", ""),
                    "prediction": prediction,
                    "extracted_answer": self._extract_prediction_label(prediction),
                    "ground_truth": ground_truth,
                    "score": score,
                })
                results["metrics"]["processed"] += 1
                
            except Exception as e:
                results["predictions"].append({
                    "question": item.get("question", ""),
                    "prediction": f"Error: {str(e)}",
                    "ground_truth": item.get("answer", ""),
                    "score": 0.0,
                })
        
        if all_scores:
            results["metrics"]["score"] = sum(all_scores) / len(all_scores)
        
        return results